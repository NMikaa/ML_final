{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# აუცილებელი ბიბლიოთეკების იმპორტი"
   ],
   "id": "d83c7747ac6d4520"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:00:33.829150Z",
     "start_time": "2024-08-09T06:00:33.787158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "fd24085d2185a25d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-09T06:00:37.604921Z",
     "start_time": "2024-08-09T06:00:33.832145Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch\n",
    "from glove import GloVe, glove_loss, build_cooccurrence_matrices, train_glove\n",
    "from glove import tokenize_text\n",
    "import random\n",
    "from glove.config import Config"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CSV ფაილის ჩატვირთვა\n",
    "აქ არის ის დატა, რაზეც უნდა დათრენინგდეს მოდელი"
   ],
   "id": "76259eb87865ee78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:00:38.398631Z",
     "start_time": "2024-08-09T06:00:37.606326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data_for_training.csv')"
   ],
   "id": "f419b89204502e2c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T22:11:32.045001Z",
     "start_time": "2024-08-06T22:11:32.031337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ტექსტის სვეტის ამოღება\n",
    "corpus = df['Text'].tolist()"
   ],
   "id": "10b6742da4123699",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T22:11:32.097471Z",
     "start_time": "2024-08-06T22:11:32.050881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random.shuffle(corpus)"
   ],
   "id": "b71345af0e4c70f6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ჰიპერპარამეტრების განსაზღვრა\n"
   ],
   "id": "fce3a4d7ad9595df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T23:44:49.900635Z",
     "start_time": "2024-08-06T23:44:49.895920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = Config() "
   ],
   "id": "1ddcc67ca1c61f5e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# დატას გამზადება"
   ],
   "id": "896dc48dc18ab6ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ვოქაბის შექმნა\n",
    "ვიყენებ ბერტის ტოკენაიზერს, რადგან ყველა სიტყვის განხილვას დიდი სფეისი მიაქვს და ვერ ვუშვებდი."
   ],
   "id": "8e869545388c389b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T23:47:13.342532Z",
     "start_time": "2024-08-06T23:47:12.499336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "id": "5099e0cd7e4327f3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T22:17:45.890947Z",
     "start_time": "2024-08-06T22:11:32.122874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_corpus = tokenize_text(corpus, tokenizer)"
   ],
   "id": "2b24b5335ec338c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12231/12231 [06:12<00:00, 32.81it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## თრეინ და ვალიდაციის სეტის გამოყოფა ემბედინგების თრენინგისთვის\n",
    "აქ ვქმნი მატრიცას რაც გადაეცემა GLoVe მოდელს სათრენინგოდ"
   ],
   "id": "48c2b6ec2d2c371e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T22:41:51.679187Z",
     "start_time": "2024-08-06T22:17:45.893851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_matrix, valid_matrix, word_to_idx = build_cooccurrence_matrices(tokenized_corpus, config.window_size)"
   ],
   "id": "ae32b6ea3f93f94f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12231/12231 [24:04<00:00,  8.47it/s] \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T22:42:14.807043Z",
     "start_time": "2024-08-06T22:41:51.691463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_matrix = train_matrix.toarray()\n",
    "valid_matrix = valid_matrix.toarray()"
   ],
   "id": "8d9dd640628ba72d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ბექაფი\n",
    "დიდი დრო უნდა მატრიცის შექმნას და რო არ მოვცდე უაზროდ \n",
    "\n"
   ],
   "id": "e545ca4c58a0b85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T22:42:56.236535Z",
     "start_time": "2024-08-06T22:42:14.854938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import pickle\n",
    "# \n",
    "# with open('train_matrix.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_matrix, f)\n",
    "# \n",
    "# with open('valid_matrix.pkl', 'wb') as f:\n",
    "#     pickle.dump(valid_matrix, f)    \n",
    "# \n",
    "# with open('word_to_idx.pkl', 'wb') as f:\n",
    "#     pickle.dump(word_to_idx, f)"
   ],
   "id": "cfaae839d1458ca4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T00:49:52.142449Z",
     "start_time": "2024-08-07T00:49:29.614852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "# \n",
    "with open('train_matrix.pkl', 'rb') as f:\n",
    "    train_matrix = pickle.load(f)\n",
    "\n",
    "with open('valid_matrix.pkl', 'rb') as f:\n",
    "    valid_matrix = pickle.load(f)   \n",
    "\n",
    "with open('word_to_idx.pkl', 'rb') as f:\n",
    "    word_to_idx = pickle.load(f)"
   ],
   "id": "3560a0d5f8c30fe",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ემბედინგების მოდელის თრენინგი"
   ],
   "id": "173c9d7979001751"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T01:58:23.939526Z",
     "start_time": "2024-08-07T00:58:34.200432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config.patience = 15\n",
    "config.learning_rate = 0.01\n",
    "model = train_glove(train_matrix, valid_matrix, config)"
   ],
   "id": "e57e5668a5fc2e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.4536, Valid Loss: 0.2166\n",
      "Epoch: 1, Train Loss: 0.4474, Valid Loss: 0.2124\n",
      "Epoch: 2, Train Loss: 0.4405, Valid Loss: 0.2077\n",
      "Epoch: 3, Train Loss: 0.4327, Valid Loss: 0.2022\n",
      "Epoch: 4, Train Loss: 0.4236, Valid Loss: 0.1958\n",
      "Epoch: 5, Train Loss: 0.4128, Valid Loss: 0.1883\n",
      "Epoch: 6, Train Loss: 0.4002, Valid Loss: 0.1798\n",
      "Epoch: 7, Train Loss: 0.3858, Valid Loss: 0.1703\n",
      "Epoch: 8, Train Loss: 0.3694, Valid Loss: 0.1598\n",
      "Epoch: 9, Train Loss: 0.3512, Valid Loss: 0.1485\n",
      "Epoch: 10, Train Loss: 0.3312, Valid Loss: 0.1366\n",
      "Epoch: 11, Train Loss: 0.3097, Valid Loss: 0.1242\n",
      "Epoch: 12, Train Loss: 0.2869, Valid Loss: 0.1118\n",
      "Epoch: 13, Train Loss: 0.2632, Valid Loss: 0.0997\n",
      "Epoch: 14, Train Loss: 0.2390, Valid Loss: 0.0882\n",
      "Epoch: 15, Train Loss: 0.2150, Valid Loss: 0.0781\n",
      "Epoch: 16, Train Loss: 0.1917, Valid Loss: 0.0697\n",
      "Epoch: 17, Train Loss: 0.1698, Valid Loss: 0.0635\n",
      "Epoch: 18, Train Loss: 0.1501, Valid Loss: 0.0601\n",
      "Epoch: 19, Train Loss: 0.1332, Valid Loss: 0.0595\n",
      "Epoch: 20, Train Loss: 0.1195, Valid Loss: 0.0616\n",
      "Epoch: 21, Train Loss: 0.1089, Valid Loss: 0.0659\n",
      "Epoch: 22, Train Loss: 0.1013, Valid Loss: 0.0717\n",
      "Epoch: 23, Train Loss: 0.0960, Valid Loss: 0.0782\n",
      "Epoch: 24, Train Loss: 0.0923, Valid Loss: 0.0844\n",
      "Epoch: 25, Train Loss: 0.0894, Valid Loss: 0.0897\n",
      "Epoch: 26, Train Loss: 0.0867, Valid Loss: 0.0935\n",
      "Epoch: 27, Train Loss: 0.0838, Valid Loss: 0.0955\n",
      "Epoch: 28, Train Loss: 0.0805, Valid Loss: 0.0957\n",
      "Epoch: 29, Train Loss: 0.0768, Valid Loss: 0.0942\n",
      "Epoch: 30, Train Loss: 0.0728, Valid Loss: 0.0914\n",
      "Epoch: 31, Train Loss: 0.0688, Valid Loss: 0.0877\n",
      "Epoch: 32, Train Loss: 0.0651, Valid Loss: 0.0833\n",
      "Epoch: 33, Train Loss: 0.0619, Valid Loss: 0.0788\n",
      "Early stopping at epoch 34\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T01:58:24.120286Z",
     "start_time": "2024-08-07T01:58:23.986929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), \"glove_model_v2.pt\")"
   ],
   "id": "d9fb60a1a6fe47c1",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T10:00:41.702907Z",
     "start_time": "2024-08-07T10:00:41.486030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GloVe(len(word_to_idx), config.embedding_dim)\n",
    "model.load_state_dict(torch.load('glove_model.pt'))"
   ],
   "id": "3e037624536dbb5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# პატარა ევალუაცია\n",
    "აქ ემბედინგ არითმეტიკას ვამოწმებ. ასევე, რადგან ტექსტის ემბედინგები გვინდა და არა სიტყვის ბოლოს, რამე გზა უნდა მოვიფიქროთ ამისთვის"
   ],
   "id": "d5cee1308d41fe88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:14:23.104801Z",
     "start_time": "2024-08-09T06:14:23.065802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_word_probabilities(train_data, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute word probabilities from the training dataset.\n",
    "\n",
    "    Parameters:\n",
    "    train_data (list of str): List of documents in the training set\n",
    "    tokenizer: Tokenizer to use for tokenizing words\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of word probabilities\n",
    "    \"\"\"\n",
    "    word_freq = Counter()\n",
    "    total_words = 0\n",
    "    for doc in tqdm(train_data):\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        word_freq.update(tokens)\n",
    "        total_words += len(tokens)\n",
    "    \n",
    "    word_prob = {word: count / total_words for word, count in word_freq.items()}\n",
    "    return word_prob\n",
    "\n",
    "def prepare_sif_embedding(train_data, tokenizer):\n",
    "    \"\"\"\n",
    "    Prepare SIF embedding by computing word probabilities from training data.\n",
    "\n",
    "    Parameters:\n",
    "    train_data (list of str): List of documents in the training set\n",
    "    tokenizer: Tokenizer to use for tokenizing words\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of word probabilities\n",
    "    \"\"\"\n",
    "    return compute_word_probabilities(train_data, tokenizer)"
   ],
   "id": "cc923199ded8722f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T00:21:06.683799Z",
     "start_time": "2024-08-07T00:21:06.655382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "def get_text_embedding(model, word_to_idx, text, tokenizer, method='simple', word_frequency=None, a=1e-3):\n",
    "    \"\"\"\n",
    "    Get the embedding for a given text using specified method.\n",
    "\n",
    "    Parameters:\n",
    "    model (GloVe): Trained GloVe model\n",
    "    word_to_idx (dict): Mapping of words to their indices\n",
    "    text (str): Input text to embed\n",
    "    tokenizer: Tokenizer to use for tokenizing words\n",
    "    method (str): Embedding method to use. Options are 'simple', 'advanced', 'tfidf', or 'sif'\n",
    "    word_frequency (dict): Dictionary of word frequencies (required for 'sif' method)\n",
    "    a (float): Parameter for SIF weighting formula\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Embedding of the text\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    if method == 'simple':\n",
    "        return simple_average_embedding(model, word_to_idx, text, tokenizer)\n",
    "    elif method == 'advanced':\n",
    "        return advanced_embedding(model, word_to_idx, text, tokenizer)\n",
    "    elif method == 'tfidf':\n",
    "        return tfidf_weighted_embedding(model, word_to_idx, text, tokenizer)\n",
    "    elif method == 'sif':\n",
    "        if word_frequency is None:\n",
    "            raise ValueError(\"word_frequency is required for SIF method\")\n",
    "        return sif_embedding(model, word_to_idx, text, tokenizer, word_frequency, a)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'simple', 'advanced', 'tfidf', or 'sif'.\")\n",
    "\n",
    "\n",
    "def simple_average_embedding(model, word_to_idx, text, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    word_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for token in tokens:\n",
    "            if token in word_to_idx:\n",
    "                word_idx = word_to_idx[token]\n",
    "                word_embedding = (model.wi.weight[word_idx] + model.wj.weight[word_idx]) / 2\n",
    "                word_embeddings.append(word_embedding)\n",
    "\n",
    "    if word_embeddings:\n",
    "        text_embedding = torch.stack(word_embeddings).mean(dim=0)\n",
    "    else:\n",
    "        text_embedding = torch.zeros(model.wi.embedding_dim)\n",
    "\n",
    "    return text_embedding\n",
    "\n",
    "def advanced_embedding(model, word_to_idx, text, tokenizer):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentence in sentences:\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            word_embeddings = []\n",
    "\n",
    "            for token in tokens:\n",
    "                if token in word_to_idx:\n",
    "                    word_idx = word_to_idx[token]\n",
    "                    word_embedding = (model.wi.weight[word_idx] + model.wj.weight[word_idx]) / 2\n",
    "                    word_embeddings.append(word_embedding)\n",
    "\n",
    "            if word_embeddings:\n",
    "                sentence_embedding = torch.stack(word_embeddings).max(dim=0)[0]\n",
    "                sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "    if sentence_embeddings:\n",
    "        text_embedding = torch.stack(sentence_embeddings).mean(dim=0)\n",
    "    else:\n",
    "        text_embedding = torch.zeros(model.wi.embedding_dim)\n",
    "\n",
    "    return text_embedding\n",
    "\n",
    "def tfidf_weighted_embedding(model, word_to_idx, text, tokenizer):\n",
    "    # Create a corpus with just the input text\n",
    "    corpus = [text]\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize, lowercase=False)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Get word embeddings and TF-IDF weights\n",
    "    word_embeddings = []\n",
    "    tfidf_weights = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for word, idx in vectorizer.vocabulary_.items():\n",
    "            if word in word_to_idx:\n",
    "                word_idx = word_to_idx[word]\n",
    "                word_embedding = (model.wi.weight[word_idx] + model.wj.weight[word_idx]) / 2\n",
    "                word_embeddings.append(word_embedding)\n",
    "                tfidf_weights.append(tfidf_matrix[0, idx])\n",
    "\n",
    "    if word_embeddings:\n",
    "        # Convert to PyTorch tensors\n",
    "        word_embeddings = torch.stack(word_embeddings)\n",
    "        tfidf_weights = torch.tensor(tfidf_weights, dtype=torch.float32)\n",
    "\n",
    "        # Normalize weights\n",
    "        tfidf_weights = tfidf_weights / tfidf_weights.sum()\n",
    "\n",
    "        # Compute weighted average\n",
    "        text_embedding = (word_embeddings * tfidf_weights.unsqueeze(1)).sum(dim=0)\n",
    "    else:\n",
    "        text_embedding = torch.zeros(model.wi.embedding_dim)\n",
    "\n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def sif_embedding(model, word_to_idx, text, tokenizer, word_frequency, a):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    word_embeddings = []\n",
    "    word_weights = []\n",
    "\n",
    "    total_words = sum(word_frequency.values())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for token in tokens:\n",
    "            if token in word_to_idx:\n",
    "                word_idx = word_to_idx[token]\n",
    "                word_embedding = (model.wi.weight[word_idx] + model.wj.weight[word_idx]) / 2\n",
    "                word_embeddings.append(word_embedding)\n",
    "                \n",
    "                # Compute SIF weight\n",
    "                word_prob = word_frequency.get(token, 1) / total_words  # use 1 if word not in frequency dict\n",
    "                weight = a / (a + word_prob)\n",
    "                word_weights.append(weight)\n",
    "\n",
    "    if word_embeddings:\n",
    "        # Convert to PyTorch tensors\n",
    "        word_embeddings = torch.stack(word_embeddings)\n",
    "        word_weights = torch.tensor(word_weights, dtype=torch.float32)\n",
    "\n",
    "        # Compute weighted average\n",
    "        text_embedding = (word_embeddings * word_weights.unsqueeze(1)).sum(dim=0) / word_weights.sum()\n",
    "    else:\n",
    "        text_embedding = torch.zeros(model.wi.embedding_dim)\n",
    "\n",
    "    return text_embedding"
   ],
   "id": "d242c96ef3ac6762",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:17:24.943451Z",
     "start_time": "2024-08-09T06:17:24.910048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    vec1 (np.ndarray): First vector.\n",
    "    vec2 (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity between vec1 and vec2.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ],
   "id": "3723b2c518586b58",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T07:15:24.189836Z",
     "start_time": "2024-08-07T07:15:24.173753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming you've already loaded your model and word_to_idx\n",
    "anchor_cv = \"John Doe is a seasoned software engineer with 8 years of experience in full-stack web development. He specializes in Python and JavaScript, with a strong background in building scalable web applications using frameworks like Django and React. John has led multiple teams in agile environments, consistently delivering projects on time and within budget. He holds a Bachelor's degree in Computer Science from MIT and has contributed to several open-source projects. In his spare time, John enjoys participating in hackathons and mentoring junior developers.\"\n",
    "positive_cv = \"Jane Smith is an accomplished software engineer with 7 years of experience in web development and cloud technologies. Her expertise lies in Python and Node.js, with extensive experience in developing robust backend systems and RESTful APIs. Jane has successfully managed cross-functional teams in fast-paced startup environments, ensuring high-quality deliverables and meeting tight deadlines. She graduated with a Master's degree in Software Engineering from Stanford University and regularly speaks at tech conferences. Outside of work, Jane is passionate about promoting diversity in tech and volunteers as a coding instructor for underrepresented groups.\"\n",
    "negative_cv = \"Dr. Emily Rodriguez is a distinguished marine biologist with 15 years of experience studying coral reef ecosystems in the Caribbean. Her groundbreaking research on the impact of climate change on marine biodiversity has been published in numerous peer-reviewed journals, including Nature and Science. Dr. Rodriguez has led several international research expeditions, collaborating with governments and NGOs to establish marine protected areas. She holds a Ph.D. in Marine Biology from the Scripps Institution of Oceanography and is a certified scuba diving instructor. In her free time, Emily is an avid underwater photographer and has had her work featured in National Geographic.\"\n"
   ],
   "id": "7782b0b21ee13391",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T10:00:58.981538Z",
     "start_time": "2024-08-07T10:00:58.666192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get embeddings\n",
    "anchor_embedding = get_text_embedding(model, word_to_idx, anchor_cv, tokenizer, method='simple')\n",
    "positive_embedding = get_text_embedding(model, word_to_idx, positive_cv, tokenizer, method='simple')\n",
    "negative_embedding = get_text_embedding(model, word_to_idx, negative_cv, tokenizer, method='simple')\n",
    "\n",
    "print(f\"ახლოს უნდა იყოს ერთთან: {cosine_similarity(anchor_embedding, positive_embedding)}\")\n",
    "print(f\"შედარებით შორს უნდა იყოს: {cosine_similarity(anchor_embedding, negative_embedding)}\")"
   ],
   "id": "dfaf9104d7f5ce11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ახლოს უნდა იყოს ერთთან: 0.999369204044342\n",
      "შედარებით შორს უნდა იყოს: 0.9875428080558777\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T10:00:59.359115Z",
     "start_time": "2024-08-07T10:00:59.000428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get embeddings\n",
    "anchor_embedding = get_text_embedding(model, word_to_idx, anchor_cv, tokenizer, method='advanced')\n",
    "positive_embedding = get_text_embedding(model, word_to_idx, positive_cv, tokenizer, method='advanced')\n",
    "negative_embedding = get_text_embedding(model, word_to_idx, negative_cv, tokenizer, method='advanced')\n",
    "\n",
    "\n",
    "print(f\"ახლოს უნდა იყოს ერთთან: {cosine_similarity(anchor_embedding, positive_embedding)}\")\n",
    "print(f\"შედარებით შორს უნდა იყოს: {cosine_similarity(anchor_embedding, negative_embedding)}\")"
   ],
   "id": "46560088e517d147",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ახლოს უნდა იყოს ერთთან: 0.9976357817649841\n",
      "შედარებით შორს უნდა იყოს: 0.9894310832023621\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T10:01:01.432706Z",
     "start_time": "2024-08-07T10:01:01.065668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get embeddings\n",
    "anchor_embedding = get_text_embedding(model, word_to_idx, anchor_cv, tokenizer, method='tfidf')\n",
    "positive_embedding = get_text_embedding(model, word_to_idx, positive_cv, tokenizer, method='tfidf')\n",
    "negative_embedding = get_text_embedding(model, word_to_idx, negative_cv, tokenizer, method='tfidf')\n",
    "\n",
    "\n",
    "print(f\"ახლოს უნდა იყოს ერთთან: {cosine_similarity(anchor_embedding, positive_embedding)}\")\n",
    "print(f\"შედარებით შორს უნდა იყოს: {cosine_similarity(anchor_embedding, negative_embedding)}\")"
   ],
   "id": "f1fd75f36738caa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darsal-brdzeni/anaconda3/envs/tiny_stories/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ახლოს უნდა იყოს ერთთან: 0.9993693232536316\n",
      "შედარებით შორს უნდა იყოს: 0.9875428080558777\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:14:34.905316Z",
     "start_time": "2024-08-09T06:14:33.034865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('./data_for_training.csv')\n",
    "train_corpus = df['Text'].tolist()\n",
    "freq = prepare_sif_embedding(train_corpus, tokenizer)"
   ],
   "id": "70d37b032745cea0",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data_for_training.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m train_corpus \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mText\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m----> 3\u001B[0m freq \u001B[38;5;241m=\u001B[39m prepare_sif_embedding(train_corpus, \u001B[43mtokenizer\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T10:01:07.345809Z",
     "start_time": "2024-08-07T10:01:07.258562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get embeddings\n",
    "anchor_embedding = get_text_embedding(model, word_to_idx, anchor_cv, tokenizer, method='sif', word_frequency=freq)\n",
    "positive_embedding = get_text_embedding(model, word_to_idx, positive_cv, tokenizer, method='sif', word_frequency=freq)\n",
    "negative_embedding = get_text_embedding(model, word_to_idx, negative_cv, tokenizer, method='sif', word_frequency=freq)\n",
    "\n",
    "\n",
    "print(f\"ახლოს უნდა იყოს ერთთან: {cosine_similarity(anchor_embedding, positive_embedding)}\")\n",
    "print(f\"შედარებით შორს უნდა იყოს: {cosine_similarity(anchor_embedding, negative_embedding)}\")"
   ],
   "id": "9d24eeeec5e2136c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ახლოს უნდა იყოს ერთთან: 0.9989663362503052\n",
      "შედარებით შორს უნდა იყოს: 0.98427414894104\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- დიდად არცერთით არ ვარ გაოცებული, მაგრამ რადგან sif-ს ყველაზე შორს ჰყავს ნეგატივი, მაგას ავირჩევ "
   ],
   "id": "9d184df863bb2682"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ახლა კი ვიწყებთ კლასიფიცაკიის მოდელის აგებას"
   ],
   "id": "ffed845c9a35888c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T10:09:39.262998Z",
     "start_time": "2024-08-07T10:01:16.933775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('data_for_training.csv')  # Replace with your actual data file\n",
    "\n",
    "def text_to_embedding(text):\n",
    "    return get_text_embedding(model, word_to_idx, text, tokenizer, method='sif', word_frequency=freq)\n",
    "\n",
    "# Calculate embeddings for each text\n",
    "print(\"Calculating embeddings...\")\n",
    "embeddings = df['Text'].progress_apply(text_to_embedding).tolist()\n",
    "X = np.array(embeddings)\n",
    "\n",
    "# Encode categories\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Category'])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "accuracy_scores = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores[model_name] = accuracy\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(f'Accuracy of {model_name} on test set: {accuracy:.2f}')\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\nOverall Results:\")\n",
    "for model_name, accuracy in accuracy_scores.items():\n",
    "    print(f'{model_name}: {accuracy:.2f}')\n",
    "\n",
    "# Find the best model\n",
    "best_model = max(accuracy_scores, key=accuracy_scores.get)\n",
    "print(f\"\\nBest model: {best_model} with accuracy {accuracy_scores[best_model]:.2f}\")"
   ],
   "id": "6f1c2487c514c549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12231/12231 [07:48<00:00, 26.11it/s]\n",
      "/home/darsal-brdzeni/anaconda3/envs/tiny_stories/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/darsal-brdzeni/anaconda3/envs/tiny_stories/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating KNN...\n",
      "Accuracy of KNN on test set: 0.41\n",
      "Training and evaluating LogisticRegression...\n",
      "Accuracy of LogisticRegression on test set: 0.27\n",
      "Training and evaluating RandomForest...\n",
      "Accuracy of RandomForest on test set: 0.50\n",
      "Training and evaluating SVM...\n",
      "Accuracy of SVM on test set: 0.26\n",
      "\n",
      "Overall Results:\n",
      "KNN: 0.41\n",
      "LogisticRegression: 0.27\n",
      "RandomForest: 0.50\n",
      "SVM: 0.26\n",
      "\n",
      "Best model: RandomForest with accuracy 0.50\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ცხოვრებას ფერი არ ჰქონია თურმე ნარინჯის...\n",
    "საკმაოდ ცუდი შედეგები აქვს როგორც ზოგადად ემბედინგებს, ისე ამ კლასიფიკაციასაც. \n",
    "ჩემი აზრით, დატა აკლია ნულიდან თრენინგისთვის."
   ],
   "id": "7d7ae1a3fccc22ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:08:42.719809Z",
     "start_time": "2024-08-09T06:07:59.281422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Load the GloVe embeddings\n",
    "def load_glove_model(glove_file):\n",
    "    glove_model = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = torch.tensor([float(val) for val in split_line[1:]], dtype=torch.float32)\n",
    "            glove_model[word] = embedding\n",
    "    return glove_model\n",
    "\n",
    "# Provide the path to your GloVe file\n",
    "glove_file = 'glove/glove.6B/glove.6B.300d.txt'\n",
    "glove_embeddings = load_glove_model(glove_file)\n",
    "\n",
    "# Step 2: Get embeddings for specific words\n",
    "def get_word_embedding(word, embeddings):\n",
    "    return embeddings.get(word, torch.zeros(300))  # Returns a zero vector if the word is not found\n",
    "\n",
    "# Example usage\n",
    "word = 'compuhhhter'\n",
    "embedding = get_word_embedding(word, glove_embeddings)\n",
    "print(embedding)"
   ],
   "id": "9248dcb3e5b88f0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:09:11.098948Z",
     "start_time": "2024-08-09T06:09:11.070826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "word = 'computer'\n",
    "embedding = get_word_embedding(word, glove_embeddings)\n",
    "print(embedding)"
   ],
   "id": "b9a12f184b160ace",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7628e-01,  1.3999e-01,  9.8519e-02, -6.4019e-01,  3.1988e-02,\n",
      "         1.0066e-01, -1.8673e-01, -3.7129e-01,  5.9740e-01, -2.0405e+00,\n",
      "         2.2368e-01, -2.6314e-02,  7.2408e-01, -4.3829e-01,  4.8886e-01,\n",
      "        -3.5486e-03, -1.0006e-01, -3.0587e-01, -1.5621e-01, -6.8136e-02,\n",
      "         2.1104e-01,  2.9287e-01, -8.8861e-02, -2.0462e-01, -5.7602e-01,\n",
      "         3.4526e-01,  4.1390e-01,  1.7917e-01,  2.5143e-01, -2.2678e-01,\n",
      "        -1.0103e-01,  1.4576e-01,  2.0127e-01,  3.1810e-01, -7.8907e-01,\n",
      "        -2.2194e-01, -2.4833e-01, -1.5103e-02, -2.0050e-01, -2.6441e-02,\n",
      "         1.8551e-01,  3.3782e-01, -3.3543e-01,  8.6117e-01, -4.7083e-02,\n",
      "        -1.7009e-01,  3.0438e-01,  9.4119e-02,  3.2435e-01, -8.1171e-01,\n",
      "         8.8966e-01, -3.9149e-01,  1.6828e-01,  1.4316e-01,  3.6339e-03,\n",
      "        -6.4557e-02,  4.5777e-02, -3.2248e-01,  4.8943e-02,  1.6817e-01,\n",
      "         6.8344e-02,  5.4227e-01,  1.2493e-01,  6.9742e-01, -3.7194e-02,\n",
      "         3.3080e-01, -4.2194e-01,  3.3970e-01,  2.7646e-01, -1.6003e-02,\n",
      "        -2.1827e-01,  4.4535e-01,  3.5379e-01, -2.2089e-02,  2.1375e-01,\n",
      "         4.3267e-01, -3.2897e-01,  9.6165e-02,  3.1265e-01, -3.0528e-01,\n",
      "         2.6126e-01, -6.5364e-01, -7.8014e-01, -2.3154e-01,  1.2113e-01,\n",
      "         3.4896e-01, -5.5444e-01,  4.6619e-01, -1.6520e-01,  1.1611e-01,\n",
      "        -7.6676e-01,  6.9502e-01, -1.5698e-01, -1.2490e-01,  5.6505e-01,\n",
      "         6.4499e-01, -5.7403e-01, -3.3549e-02,  3.2898e-01, -1.4025e+00,\n",
      "        -3.1143e-01,  6.4549e-01, -6.1534e-02, -6.9295e-01,  6.0894e-04,\n",
      "        -5.6544e-01,  1.9181e-01, -1.9208e-01, -6.2673e-01, -9.7473e-03,\n",
      "        -5.5040e-01, -5.6128e-01, -1.9603e-01,  2.9254e-01,  9.8576e-02,\n",
      "        -5.9395e-02,  3.3616e-03,  1.9515e-01, -6.0703e-01,  3.4262e-01,\n",
      "         9.5211e-02, -7.9411e-02,  1.4305e-01, -5.6569e-01, -6.5887e-02,\n",
      "         1.5167e-01, -1.3505e-01,  1.9571e-01,  2.2812e-01,  3.5346e-02,\n",
      "        -2.2509e-01,  1.8910e-01, -3.7348e-01,  1.2505e-01,  4.6249e-01,\n",
      "        -3.2219e-01,  9.0643e-01,  1.1595e-01,  1.1628e-01,  2.2961e-01,\n",
      "         2.4010e-01, -6.1609e-02,  3.9325e-01, -6.5066e-02,  4.2257e-01,\n",
      "         5.6880e-01,  4.9804e-01, -6.1308e-01,  4.1468e-01, -1.3448e-01,\n",
      "         6.0430e-01, -6.5462e-02, -8.5376e-02,  1.9115e-01,  3.9925e-01,\n",
      "         3.7495e-01, -1.8492e-01,  6.1751e-02, -3.8747e-01, -3.0335e-01,\n",
      "        -3.8211e-01,  2.8221e-01, -1.0286e-01, -5.8660e-01,  8.2922e-01,\n",
      "         2.5131e-01,  2.4772e-01,  8.7482e-01, -3.1359e-01,  8.1621e-01,\n",
      "        -9.0081e-01, -7.7933e-01, -1.0090e+00,  3.6472e-01, -1.1562e-01,\n",
      "        -2.4841e-01,  9.4527e-02, -4.2266e-01,  6.0392e-02, -1.5365e-01,\n",
      "        -6.9604e-02,  5.1292e-03,  3.9572e-01, -1.5692e-01,  3.5708e-01,\n",
      "        -3.5165e-01,  3.5296e-01, -5.2220e-01,  5.1400e-01, -1.7764e-01,\n",
      "        -1.0272e-01, -3.9640e-01,  3.0418e-01,  7.3659e-02, -1.1685e-01,\n",
      "         1.4299e-01, -3.6810e-01,  2.7642e-01, -4.6683e-01, -3.2633e-01,\n",
      "         5.1107e-01,  2.3945e-02,  1.1723e-01,  2.1761e-01, -1.7389e-01,\n",
      "        -6.1193e-01, -5.9449e-01,  4.7749e-01, -5.9008e-01, -3.6092e-01,\n",
      "        -9.9574e-02, -4.3098e-02, -1.5106e-01, -1.4336e-01, -3.1135e-02,\n",
      "         1.7887e-01, -6.4221e-01,  1.7242e-01,  3.3916e-01,  8.7181e-01,\n",
      "        -7.7230e-01,  5.3195e-01, -5.2763e-01,  1.7510e-01,  3.1043e-01,\n",
      "        -1.5177e-01, -2.2706e-01,  1.0803e-01,  4.4919e-01,  7.0016e-02,\n",
      "         2.0851e-01,  2.1517e-01, -6.1712e-01, -9.9970e-02,  5.5020e-03,\n",
      "         7.6786e-02,  2.8046e-01,  4.2331e-01, -5.8925e-01,  7.0554e-02,\n",
      "         3.9923e-01,  9.0201e-02,  1.7139e-01, -1.7282e-01, -5.3675e-01,\n",
      "        -4.6439e-01, -5.7850e-01, -6.8311e-01,  5.9383e-02,  1.2427e-01,\n",
      "        -1.4558e-01,  5.7687e-01, -5.7499e-01, -5.1645e-02,  3.8410e-01,\n",
      "         1.3047e-01,  3.3786e-01,  3.3204e-01,  4.0119e-01,  2.6389e-01,\n",
      "        -3.6953e-01, -2.9797e-01, -6.6816e-01, -1.1883e-01,  5.0133e-01,\n",
      "         2.0603e-01, -3.2558e-01, -1.2242e-01,  5.0666e-01,  1.6353e-01,\n",
      "        -1.0672e-01,  2.2364e-01,  2.3915e-01, -5.5509e-01, -4.8432e-01,\n",
      "        -1.2165e-02, -1.7992e+00,  3.2310e-01, -2.6309e-01, -3.2538e-01,\n",
      "        -5.8270e-01,  1.5099e-01,  3.3838e-01,  1.2007e-01,  4.1395e-01,\n",
      "        -1.5553e-01, -1.9301e-01,  5.8860e-02, -5.2420e-01, -3.7170e-01,\n",
      "         5.6205e-01, -6.5801e-01, -4.9796e-01,  2.4347e-01,  1.2873e-01,\n",
      "         3.3665e-01, -7.2609e-02, -1.5686e-01, -1.4187e-01, -2.6488e-01])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:18:16.344437Z",
     "start_time": "2024-08-09T06:18:16.298564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "def get_text_embedding(text, method='simple', word_frequency=None, a=1e-3):\n",
    "    \"\"\"\n",
    "    Get the embedding for a given text using specified method.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text to embed\n",
    "    method (str): Embedding method to use. Options are 'simple', 'advanced', or 'sif'\n",
    "    word_frequency (dict): Dictionary of word frequencies (required for 'sif' method)\n",
    "    a (float): Parameter for SIF weighting formula\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Embedding of the text\n",
    "    \"\"\"\n",
    "\n",
    "    if method == 'simple':\n",
    "        return simple_average_embedding(text)\n",
    "    elif method == 'advanced':\n",
    "        return advanced_embedding(text)\n",
    "    elif method == 'sif':\n",
    "        if word_frequency is None:\n",
    "            raise ValueError(\"word_frequency is required for SIF method\")\n",
    "        return sif_embedding(text, word_frequency, a)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'simple', 'advanced', 'tfidf', or 'sif'.\")\n",
    "\n",
    "\n",
    "def simple_average_embedding(text:str):\n",
    "    tokens = text.lower().split()\n",
    "    word_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for token in tokens:\n",
    "            word_embedding = get_word_embedding(token, glove_embeddings)\n",
    "            word_embeddings.append(word_embedding)\n",
    "\n",
    "    if word_embeddings:\n",
    "        text_embedding = torch.stack(word_embeddings).mean(dim=0)\n",
    "    else:\n",
    "        text_embedding = torch.zeros(model.wi.embedding_dim)\n",
    "\n",
    "    return text_embedding\n",
    "\n",
    "def advanced_embedding(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.lower().split()\n",
    "            word_embeddings = []\n",
    "\n",
    "            for token in tokens:\n",
    "                word_embedding = get_word_embedding(token, glove_embeddings)\n",
    "                word_embeddings.append(word_embedding)\n",
    "\n",
    "            if word_embeddings:\n",
    "                sentence_embedding = torch.stack(word_embeddings).max(dim=0)[0]\n",
    "                sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "    if sentence_embeddings:\n",
    "        text_embedding = torch.stack(sentence_embeddings).mean(dim=0)\n",
    "    else:\n",
    "        text_embedding = torch.zeros(model.wi.embedding_dim)\n",
    "\n",
    "    return text_embedding\n",
    "\n",
    "\n",
    "def sif_embedding(text, word_frequency, a):\n",
    "    tokens = text.lower().split()\n",
    "    word_embeddings = []\n",
    "    word_weights = []\n",
    "\n",
    "    total_words = sum(word_frequency.values())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for token in tokens:\n",
    "            word_embedding = get_word_embedding(token, glove_embeddings)\n",
    "            word_embeddings.append(word_embedding)\n",
    "            \n",
    "            # Compute SIF weight\n",
    "            word_prob = word_frequency.get(token, 1) / total_words  # use 1 if word not in frequency dict\n",
    "            weight = a / (a + word_prob)\n",
    "            word_weights.append(weight)\n",
    "\n",
    "    if word_embeddings:\n",
    "        # Convert to PyTorch tensors\n",
    "        word_embeddings = torch.stack(word_embeddings)\n",
    "        word_weights = torch.tensor(word_weights, dtype=torch.float32)\n",
    "\n",
    "        # Compute weighted average\n",
    "        text_embedding = (word_embeddings * word_weights.unsqueeze(1)).sum(dim=0) / word_weights.sum()\n",
    "    else:\n",
    "        text_embedding = torch.zeros(model.wi.embedding_dim)\n",
    "\n",
    "    return text_embedding"
   ],
   "id": "c7f83b125b32872b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:18:28.455397Z",
     "start_time": "2024-08-09T06:18:28.419040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming you've already loaded your model and word_to_idx\n",
    "anchor_cv = \"John Doe is a seasoned software engineer with 8 years of experience in full-stack web development. He specializes in Python and JavaScript, with a strong background in building scalable web applications using frameworks like Django and React. John has led multiple teams in agile environments, consistently delivering projects on time and within budget. He holds a Bachelor's degree in Computer Science from MIT and has contributed to several open-source projects. In his spare time, John enjoys participating in hackathons and mentoring junior developers.\"\n",
    "positive_cv = \"Jane Smith is an accomplished software engineer with 7 years of experience in web development and cloud technologies. Her expertise lies in Python and Node.js, with extensive experience in developing robust backend systems and RESTful APIs. Jane has successfully managed cross-functional teams in fast-paced startup environments, ensuring high-quality deliverables and meeting tight deadlines. She graduated with a Master's degree in Software Engineering from Stanford University and regularly speaks at tech conferences. Outside of work, Jane is passionate about promoting diversity in tech and volunteers as a coding instructor for underrepresented groups.\"\n",
    "negative_cv = \"Dr. Emily Rodriguez is a distinguished marine biologist with 15 years of experience studying coral reef ecosystems in the Caribbean. Her groundbreaking research on the impact of climate change on marine biodiversity has been published in numerous peer-reviewed journals, including Nature and Science. Dr. Rodriguez has led several international research expeditions, collaborating with governments and NGOs to establish marine protected areas. She holds a Ph.D. in Marine Biology from the Scripps Institution of Oceanography and is a certified scuba diving instructor. In her free time, Emily is an avid underwater photographer and has had her work featured in National Geographic.\"\n"
   ],
   "id": "c269ecc3ac635e0f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:18:29.160424Z",
     "start_time": "2024-08-09T06:18:29.119420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get embeddings\n",
    "anchor_embedding = get_text_embedding(anchor_cv, method='simple')\n",
    "positive_embedding = get_text_embedding(positive_cv , method='simple')\n",
    "negative_embedding = get_text_embedding(negative_cv, method='simple')\n",
    "\n",
    "print(f\"ახლოს უნდა იყოს ერთთან: {cosine_similarity(anchor_embedding, positive_embedding)}\")\n",
    "print(f\"შედარებით შორს უნდა იყოს: {cosine_similarity(anchor_embedding, negative_embedding)}\")"
   ],
   "id": "3994e13f2343642f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ახლოს უნდა იყოს ერთთან: 0.9673110246658325\n",
      "შედარებით შორს უნდა იყოს: 0.906856119632721\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:18:30.023227Z",
     "start_time": "2024-08-09T06:18:29.972533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get embeddings\n",
    "anchor_embedding = get_text_embedding(anchor_cv, method='advanced')\n",
    "positive_embedding = get_text_embedding(positive_cv , method='advanced')\n",
    "negative_embedding = get_text_embedding(negative_cv, method='advanced')\n",
    "\n",
    "print(f\"ახლოს უნდა იყოს ერთთან: {cosine_similarity(anchor_embedding, positive_embedding)}\")\n",
    "print(f\"შედარებით შორს უნდა იყოს: {cosine_similarity(anchor_embedding, negative_embedding)}\")"
   ],
   "id": "f8f2db63986a0e35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ახლოს უნდა იყოს ერთთან: 0.981062650680542\n",
      "შედარებით შორს უნდა იყოს: 0.9587632417678833\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:21:15.585167Z",
     "start_time": "2024-08-09T06:21:15.550528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_word_probabilities(train_data):\n",
    "    \"\"\"\n",
    "    Compute word probabilities from the training dataset.\n",
    "\n",
    "    Parameters:\n",
    "    train_data (list of str): List of documents in the training set\n",
    "    tokenizer: Tokenizer to use for tokenizing words\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of word probabilities\n",
    "    \"\"\"\n",
    "    word_freq = Counter()\n",
    "    total_words = 0\n",
    "    for doc in tqdm(train_data):\n",
    "        tokens = doc.lower().split()\n",
    "        word_freq.update(tokens)\n",
    "        total_words += len(tokens)\n",
    "    \n",
    "    word_prob = {word: count / total_words for word, count in word_freq.items()}\n",
    "    return word_prob\n",
    "\n",
    "\n",
    "def prepare_sif_embedding(train_data):\n",
    "    \"\"\"\n",
    "    Prepare SIF embedding by computing word probabilities from training data.\n",
    "\n",
    "    Parameters:\n",
    "    train_data (list of str): List of documents in the training set\n",
    "    tokenizer: Tokenizer to use for tokenizing words\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary of word probabilities\n",
    "    \"\"\"\n",
    "    return compute_word_probabilities(train_data)\n",
    "\n"
   ],
   "id": "e6e3b1ded44ea2c5",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:21:21.915589Z",
     "start_time": "2024-08-09T06:21:19.293512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('./data_for_training.csv')\n",
    "train_corpus = df['Text'].tolist()\n",
    "freq = prepare_sif_embedding(train_corpus)"
   ],
   "id": "78fe984aebf180d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12231/12231 [00:01<00:00, 7801.69it/s]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:21:23.428942Z",
     "start_time": "2024-08-09T06:21:23.356576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get embeddings\n",
    "anchor_embedding = get_text_embedding(anchor_cv, method='sif', word_frequency=freq)\n",
    "positive_embedding = get_text_embedding(positive_cv , method='sif', word_frequency=freq)\n",
    "negative_embedding = get_text_embedding(negative_cv, method='sif', word_frequency=freq)\n",
    "\n",
    "print(f\"ახლოს უნდა იყოს ერთთან: {cosine_similarity(anchor_embedding, positive_embedding)}\")\n",
    "print(f\"შედარებით შორს უნდა იყოს: {cosine_similarity(anchor_embedding, negative_embedding)}\")"
   ],
   "id": "5df2496d3f3d3f56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ახლოს უნდა იყოს ერთთან: 0.8548623323440552\n",
      "შედარებით შორს უნდა იყოს: 0.6479909420013428\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# აქაც მგონი sif ყველაზე კარგ შედეგს დებს"
   ],
   "id": "355879bf277228e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T06:24:23.657523Z",
     "start_time": "2024-08-09T06:22:22.020581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('data_for_training.csv')  # Replace with your actual data file\n",
    "\n",
    "def text_to_embedding(text):\n",
    "    return get_text_embedding(text, method='sif', word_frequency=freq)\n",
    "\n",
    "# Calculate embeddings for each text\n",
    "print(\"Calculating embeddings...\")\n",
    "embeddings = df['Text'].progress_apply(text_to_embedding).tolist()\n",
    "X = np.array(embeddings)\n",
    "\n",
    "# Encode categories\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Category'])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "accuracy_scores = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores[model_name] = accuracy\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(f'Accuracy of {model_name} on test set: {accuracy:.2f}')\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\nOverall Results:\")\n",
    "for model_name, accuracy in accuracy_scores.items():\n",
    "    print(f'{model_name}: {accuracy:.2f}')\n",
    "\n",
    "# Find the best model\n",
    "best_model = max(accuracy_scores, key=accuracy_scores.get)\n",
    "print(f\"\\nBest model: {best_model} with accuracy {accuracy_scores[best_model]:.2f}\")"
   ],
   "id": "8bee598e6b049eca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12231/12231 [00:51<00:00, 239.03it/s]\n",
      "/home/darsal-brdzeni/anaconda3/envs/tiny_stories/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/home/darsal-brdzeni/anaconda3/envs/tiny_stories/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating KNN...\n",
      "Accuracy of KNN on test set: 0.58\n",
      "Training and evaluating LogisticRegression...\n",
      "Accuracy of LogisticRegression on test set: 0.68\n",
      "Training and evaluating RandomForest...\n",
      "Accuracy of RandomForest on test set: 0.65\n",
      "Training and evaluating SVM...\n",
      "Accuracy of SVM on test set: 0.66\n",
      "\n",
      "Overall Results:\n",
      "KNN: 0.58\n",
      "LogisticRegression: 0.68\n",
      "RandomForest: 0.65\n",
      "SVM: 0.66\n",
      "\n",
      "Best model: LogisticRegression with accuracy 0.68\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# საბოლოო შედეგი 68% არის\n",
    "## შეჯამება\n",
    "როგორც ჩანს გლოვს ბევრი დატა სჭირდება დასათრენინგებლად.\n",
    "ეს მოდელი ბოლოს რაც გამოვიყენე ამათი ყველაზე ცოტა დატაზე დათრენინგებული იყო, რაც 6 მილიარდი ტოკენია."
   ],
   "id": "e68e6e1ca5d34824"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
